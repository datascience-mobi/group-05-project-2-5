---
title: "Multiple and Linear Regressions (extended)"
author: "Florencia Zúñiga"
date: "7/9/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Multiple and Linear Regressions

#Table of contents

[1. LOADING DATA](#anchor1)

* [1.1. Biomarkers](#anchor2)

[2. PREPARING THE DATA](#anchor3)

* [2.1 Creating the necessary tables (Copynumber with biomarkers/all genes and Cell lines with drug sensitivity) ](#anchor4)

[3. SIMPLE LINEAR REGRESSION WITH ALL GENES: Drug sensitivity with doubling time](#anchor5)

* [3.1. Plots and visualization: Predicting how fit linear regression will be as a model to describe our data](#anchor6)

* [3.2. Linear Regression](#anchor7)
       
* [3.3. Checking the normalization of residuals ](#anchor8)

* [3.4. Visualization: Plots that describe the linear regression ](#anchor9)

[4. SIMPLE LINEAR REGRESSION WITH 100 BIOMARKERS: Drug sensitivity with copynumber](#anchor10)

* [4.1. Plots and visualization: Predicting how fit linear regression will be as a model to describe our data](#anchor11)

* [4.2. Linear Regression](#anchor12)
       
* [4.3. Checking the normalization of residuals ](#anchor13)

* [4.4. Visualization: Plots that describe the linear regression ](#anchor14)

[5.SIMPLE LINEAR REGRESSION WITH ALL GENES: Drug sensitivity with copynumber?](#anchor15)

* [5.1. Plots and visualization: Predicting how fit linear regression will be as a model to describe our data](#anchor16)

* [5.2. Linear Regression](#anchor17)
       
* [5.3. Checking the normalization of residuals ](#anchor18)

* [5.4. Visualization: Plots that describe the linear regression ](#anchor19)

[6.MULTIPLE REGRESSION WITH ALL GENES: Drug sensitivity with doubling time and copynumber](#anchor20)

* [6.1. Plots and visualization: Predicting how fit linear regression will be as a model to describe our data](#anchor21)

* [6.2. Linear Regression](#anchor22)
       
* [6.3. Checking the normalization of residuals ](#anchor23)

* [6.4. Visualization: Plots that describe the linear regression ](#anchor24)

[7.General Conclusions](#anchor25)

[8.Using PCA to determine independent variables](#anchor26)

* [8.1. PCA](#anchor27)

* [8.2. A little correlation exploration](#anchor28)
       
* [8.3. Linear Regression ](#anchor29)

[9.REMOVING THE OUTLIERS: Simple linear regression of all genes for drug sensitivity using doubling time](#anchor30)

* [9.1. Boxplot: removing the outliers](#anchor31)

* [9.2. Linear Regression removing the outliers](#anchor32)
       
* [9.3. Checking the normalization of residuals](#anchor33)

* [9.4. Visualization: Plots that describe the linear regression](#anchor34)

* [9.5. General Conclusions](#anchor35)

[10.Comparing the results for the linear regression of drug sensitivity vs copynumber using only the biomarkers wvs using all genes](#anchor36)

[11. Linear Regression with specific genes: Drug Sensitivity vs Copynumber](#anchor37)

* [11.1. Linear Regression with top 3 biomarkers](#anchor38)

* [11.2. Linear Regression with 3 non-biomarker genes](#anchor39)
       
* [11.3. Linear Regression with genes with the highest and lowest mean values for copynumber](#anchor40)

       + [11.3.1. Top 3 mean values for Copynumber](#anchor41)
       
       + [11.3.2. Lowest 3 mean values for copynumber](#anchor42)
       
[12. Table with summary](#anchor43)




# **PART 1**  

#  1.  LOADING DATA (#anchor1)                                                                                                     

Needed libraries:
```{r}
library(readr)
library(rstudioapi)
```


```{r,include = FALSE}

wd = ("/GitHub/project-02-group-05")
```


Reading the data
```{r}
Untreated <- readRDS(paste0(wd,"/data/NCI_TPW_gep_untreated.rds"))
Treated <- readRDS(paste0(wd,"/data/NCI_TPW_gep_treated.rds"))

Metadata = read.table(paste0(wd,"/data/NCI_TPW_metadata.tsv"), 
                      header = TRUE, sep ="\t", stringsAsFactors = TRUE)

Sensitivity <- readRDS(paste0(wd,"/data/NegLogGI50.rds"))


Basal <- readRDS(paste0(wd,"/data/CCLE_basalexpression.rds"))
Copynumber <- readRDS(paste0(wd,"/data/CCLE_copynumber.rds"))
Mutations <- readRDS(paste0(wd,"/data/CCLE_mutations.rds"))

Cellline_annotation = read.table(paste0(wd,"/data/cellline_annotation.tsv"), 
                                 header = TRUE, sep ="\t", stringsAsFactors = TRUE)
Drug_annotation = read.table(paste0(wd,"/data/drug_annotation.tsv"), 
                             header = TRUE, sep ="\t", stringsAsFactors = TRUE)
```

Transforming the data
```{r}
Treated <- as.data.frame(Treated)
Untreated <- as.data.frame(Untreated)
Sensitivity<- as.data.frame(Sensitivity)
```

Data normalization
```{r}
Untreated_norm <- apply(Untreated, 2, function(x){
  (x - mean(x)) / sd(x)
 })


Treated_norm <- apply(Treated, 2, function(x){
  (x - mean(x)) / sd(x)
 })


FC <- Treated - Untreated
FC_norm <- apply(FC, 2, function(x){
  (x - mean(x)) / sd(x)
 })
```

##   1.1 Biomarkers (#anchor2)                                                                                                 
###  (1)  Creating Vorinostat
```{r}
#Untreated matrix
UntreatedVorinostatcolumns <- grep(pattern = "vorinostat",colnames(Untreated))

#Same with treated matrix
TreatedVorinostatcolumns <- grep(pattern = "vorinostat",colnames(Treated))

#Define Vorinostat-data: 
UntreatedVorinostat <- Untreated[,UntreatedVorinostatcolumns]
TreatedVorinostat <- Treated[,TreatedVorinostatcolumns]

#fold change matrix 
FC <- TreatedVorinostat - UntreatedVorinostat

#Sensitivity 
vorinostat_Sensitivity_alleZeilen= grep ('vorinostat', rownames(Sensitivity))
vorinostat_Sensitivity_data= Sensitivity[vorinostat_Sensitivity_alleZeilen,]
```

###  (2)  Creating FC Data -  Finding the Biomarkers

```{r}
FC <- TreatedVorinostat - UntreatedVorinostat

#We work with mean of the rows because we only want to compare the genes 
FC_meanrow= rowMeans(FC)

## Sorting the data 
#We work with absolute value to find the highest values, 
#because we want to have the most up and down regulated genes.
FC_abs= abs(FC_meanrow)

#We sort the values to get the 100 largest values 
sortedFC_abs <- sort(FC_abs, decreasing = TRUE)
sortedFC_abs <- as.matrix(sortedFC_abs)

#We select the first n for biomarkers 
biomarkers_FC30 = sortedFC_abs[1:30,]
biomarkers_FC30 <- as.matrix(biomarkers_FC30)

biomarkers_FC100 = sortedFC_abs[1:100,]
biomarkers_FC100 <- as.matrix(biomarkers_FC100)

## Creating a matrix with FC values, that are both positive and negative 
FC_both= cbind(FC_meanrow,FC_abs)
FC_both=as.data.frame(FC_both)

#Ordering this matrix 
FC_both_sorted <- FC_both[order(FC_both$FC_abs, decreasing = TRUE),]

#FC values of biomarkers: We select the first 100 of the sorted matrix. 
biomarkers_FC_values30 = FC_both_sorted[1:30,]

biomarkers_FC_values100 = FC_both_sorted[1:100,]

#Removing the absolute values
biomarkers_FC_values30 <- subset( biomarkers_FC_values30, select = -FC_abs)
biomarkers_FC_values30 = as.matrix(biomarkers_FC_values30)

biomarkers_FC_values100 <- subset( biomarkers_FC_values100, select = -FC_abs)
biomarkers_FC_values100 = as.matrix(biomarkers_FC_values100)

```


#  2.  PREPARING THE DATA         (#anchor3)                                                                                       
##   2.1 Creating the necessary tables (Copynumber with biomarkers/all genes and Cell lines with drug sensitivity)   (#anchor4)      

### (1) Table 1: Selection of 100 Biomarkers in copynumber 

```{r}
BM_copynumber = Copynumber[ which(row.names(Copynumber) 
                                  %in% rownames(biomarkers_FC_values100)), ]

BM_Copynumber_meancol= colMeans(BM_copynumber)

CN = as.data.frame(BM_Copynumber_meancol)
```

### (2) Table 2: All genes in copynumber 

```{r}
CN_meancol= colMeans(Copynumber)

CN_all = as.data.frame(CN_meancol)
```


### (3) Table 3: Selection of Doubling time from cellline_annotation

```{r}
#Loading package
library(dplyr)

#Selecting the desired columns
Doubling_Time <- Cellline_annotation %>% 
  select(Cell_Line_Name, Doubling_Time)

#Changig the name of the "name" column to the names of the cell lines

row.names(Doubling_Time) <- Doubling_Time$Cell_Line_Name
Doubling_Time[1] <- NULL
```


### (4) Table 4: Selection of vorinostat row from Sensitivity

```{r}
drug_sensitivity <- Sensitivity[-c(1:14),] 

drug_sensitivity = t(drug_sensitivity)
```

#  3.  SIMPLE LINEAR REGRESSION WITH ALL GENES: Drug sensitivity with doubling time      (#anchor5)                                

Can we predict drug sensitivity using doubling time? How much of the variance of the data can be explained using the doubling time?

### Data frames

```{r}
DT = as.data.frame(Doubling_Time)

DS = as.data.frame(drug_sensitivity)
```


### Table with drug sensitivity and doubling time per cell line

```{r}
lm_tab = transform(merge(DT,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)
```


##   3.1 Plots and visualization: Predicting how fit linear regression will be as a model to describe our data              


Ploting the data: can a linear relationship be observed? Should we expect a high value for R-squared?

Here we aim to Visualize the relationship between doubling time and drug sensitivity

#### (1) Scatter Plot

```{r}
scatter.smooth(lm_tab$vorinostat, 
               lm_tab$Doubling_Time, 
               col = "dodgerblue1",
               main = "Drug sensitivity & Doubling time Regression",
               xlab = "Drug sensitivity",
               ylab = "Doubling time",
               cex = 1.3,
               pch = 1)
```


At first look, the points in the plot are so scattered, that a linear relationship seems unlikely.  The second problem that one can observe in this graphic, is that the line describing the behaviour is not straight. 

#### (2) Box plot

```{r}
par(mfrow=c(1, 2))
boxplot(lm_tab$vorinostat, 
        main="Drug sensitivity")
        
boxplot(lm_tab$Doubling_Time, 
        main="Doubling time") 
```

A boxplot can help us visualize the amount of outliers in our data. This is relevant as too many (extreme) outliers can have great impact on the results of our analysis and can change the outcome completely. They can easily affect the slope. 
Two outliers can be observed. Considering that the table lm_tab describes the behaviour of 61 cell lines, these two outliers correspond approximately only to a 3.28% of our data. This is of course not much. But how much can that 3.28% affect our end results? What impact do the outliers have on the fit of the linear model? Would we have better results if we just removed the outliers? These are all questions we explore in the final part of the exploration related to linear regression, as these are interesting questions for an analysis of statistical nature such as this one.

#### (3) Density: Should we expect normality for drug sensitivity?

```{r}
library(e1071)

par(mfrow=c(1, 2)) 

plot(density(lm_tab$vorinostat), 
     main="Density Plot: Drug Sensitivity", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab$vorinostat), 2))
     )

polygon(density(lm_tab$vorinostat), col="royalblue1")

plot(density(lm_tab$Doubling_Time), 
     main="Density Plot: Doubling time", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab$Doubling_Time), 2))
    )

polygon(density(lm_tab$Doubling_Time), col="skyblue1")
```

Skewness of the plot on the left: 0.22 -> Plot is slightly skewed to the right

Skewness of the plot on the right: 1.03 -> Plot is skewed to the right.

The shape of a density plot and its skewness, give us an idea of how well normalized our data is. Taking this into consideration, it is worth noting that the plot for doubling time is considerably skewed to the right. This tells us our data is not very well normalized.

#### (4) Correlation: what is the level level of linear dependence between the two variables?

```{r}
cor(lm_tab$vorinostat, lm_tab$Doubling_Time)
```

A good value for correlation lies close to 1 or -1, whilst the value 0 is undesirable. Values closer to 0 indicate that there is a weak relationship. The results shows us that there is a weak correlation with a negative slope.   


These analysis help us predcit whether a linear regression is or not the best model to describe our data. 
Taking into consideration all results so far for this part, it is not unreasonable to predict that a linear regression will probably not be the best model to describe the relationships in our data.

##   3.2 Linear Regression                                                                                                 ####

This is the function for a linear regression:

linear_regression = lm(predicted ~ predictor, data = dat) 


### Linear Regression

```{r}
reg1 <- lm(vorinostat ~ Doubling_Time, data = lm_tab)
```


### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg1)
```


Multiple R-squared:  0.04235 
This indicates that only 4,235% percent of the variation in the data (drug sensitivity) can be explained by the relationship between drug sensitivity and doubling time. In other words, there is a 4.235% variance reduction when we take the doubling time into account. 

p-value: 0.1116 
As the p-value for reg1 is significantly larger than 0.05 and R-squared tells us the doubling time only explains 4.235% of the variation in the data, it is safe to assume that there is no linear relationship between drug sensitivity and doubling time, a.k.a doubling time cannot predict drug sensitivity. 

More information about the fit (linear ecuation: y = y-intercept + slope * x) can be found by checking the confidence interval.
```{r}
confint(reg1)
```

We can also visualize the results of the confidence interval
```{r}
library(ggplot2)

ggplot(data = lm_tab, aes(x = Doubling_Time, y = vorinostat)) +
  geom_point() +
  stat_smooth(method = "lm", col = "slateblue3", level = 0.975) +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Linear Model Fitted to Data") 
```

The grey area represents the 97.5% confidence interval. This tells us the probability that the true value we want to find is within this grey area with a 97.5% chance. As the doubling time increases, the uncertainty gets higher. Eventhough many points are inside the grey area, there are many more that are scattered across the graph. 


##   3.3 Checking the normalization of residuals                                                                           

Here we explore two ways to visualize the normalization of residuals.
```{r}
hist(reg1$residuals, 
     breaks = 20, 
     xlab = "Residuals", 
     main = "Drug sensitivity vs doubling time: Histogram of the residuals")
```

The data does NOT look normally distributed: As the residuals are not normally distributed, then the hypothesis that they are a random dataset, takes the value NO. This means that the linear regression model does not explain all trends in the dataset.



```{r}
qqnorm(reg1$residuals, col = "royalblue2", main = "Q-Q plot: Drug sensitivity vs Doubling time")
qqline(reg1$residuals, col = "red")
```

Here we expect to be able to draw a straight line that is fitting for most points. Although, this graphic seems promising, too many points would not be properly fit. Around both ends of the lines, there are many points that are not following it properly.


##   3.4 Visualization: Plots that describe the linear regression                                                          ####

### Residual diagnostics: are the various assumptions that underpin linear regression reasonable for our data?

```{r}
library(lattice)

xyplot(resid(reg1) ~ fitted(reg1),
       xlab = "Fitted Values",
       ylab = "Residuals",
       main = "Residual Diagnostic Plot",
       col = "slateblue3",
       panel = function(x, y, ...)
       {
         panel.grid(h = -1, v = -1)
         panel.abline(h = 0)
         panel.xyplot(x, y, ...)
       }
      )
```


#### Visualization of regression 

```{r}
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2))
plot(reg1, which = c(1, 2))
```

In these graphs, we can observe the behaviour of 3 cell lines under a linear regression model. The red line on the graph on the left lets us se that a linear relationship is unlikely. 

#### Comparing prediction and real values for drug sensitivity

```{r}
plot(lm_tab$vorinostat, 
     reg1$fitted.values, 
     pch = 20, 
     col = "royalblue1", 
     xlab = "Real values", 
     ylab = "Predicted values")
abline(0, 1, col = "red")
```

The data points are quite scattered.

#  4.  SIMPLE LINEAR REGRESSION WITH 100 BIOMARKERS: Drug sensitivity with copynumber


Can we predict drug sensitivity using the copynumber data? How much of the variance of the data can be explained using the copynumber data?


### Data frames

```{r}
CN = as.data.frame(BM_Copynumber_meancol)

DS = as.data.frame(drug_sensitivity)
```


### Table with drug sensitivity and doubling time per cell line

```{r}
lm_tab2 = transform(merge(CN,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)
lm_tab2 <- na.omit(lm_tab2)
```

Because not all values included in copynumber are in drug sensitivity, the NAs are omitted. 


##   4.1 Plots and visualization: Predicting how fit linear regression will be as a model to describe our data             #### 


Ploting the data: can a linear relationship be observed? Should we have expected a high value for R-squared?

#### Visualization of doubling time vs drug sensitivity

#### (1) Scatter Plot

```{r}
scatter.smooth(lm_tab2$vorinostat, 
              lm_tab2$BM_Copynumber_meancol, 
              col = "yellowgreen",
              main = "Drug sensitivity & Copynumber Regression",
              xlab = "Drug sensitivity",
              ylab = "Copynumber",
              cex = 1.3,
              pch = 1)
```

At first look, the points in the plot are so scattered, that a linear relationship seems unlikely. The second problem that one can observe in this graphic, is that the line describing the behaviour is not straight. 


#### (2) Box plot

```{r}
par(mfrow=c(1, 2))
boxplot(lm_tab2$vorinostat, 
        main="Drug sensitivity") 
boxplot(lm_tab2$BM_Copynumber_meancol, 
        main="Copynumber") 
```

No outliers can be observed. This is good, because there is no extreme data that can affect the slope for the linear regression.

#### (3) Density: Should we expect normality for drug sensitivity?

```{r}
library(lattice)

par(mfrow=c(1, 2)) 

plot(density(lm_tab2$vorinostat), 
     main="Density Plot: Drug Sensitivity", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab2$vorinostat), 2))
    )

polygon(density(lm_tab2$vorinostat), col="springgreen3")

plot(density(lm_tab2$BM_Copynumber_meancol), 
     main="Density Plot: Copynumber", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab2$BM_Copynumber_meancol), 2))
    )

polygon(density(lm_tab2$BM_Copynumber_meancol), col="olivedrab1")
```

Skewness of the plot on the left: 0.03 -> Plot is very slightly skewed to the right.
Skewness of the plot on the right: -0.07 -> Plot is very slightly skewed to the left.

The data seems to be well normalized in terms of skewness, but the shape of the plots does not match very well that of a classic normalized density plot. 

#### (4) Correlation: what is the level level of linear dependence between the two variables?

```{r}
cor(lm_tab2$vorinostat, lm_tab2$BM_Copynumber_meancol)
```

The value here is extremely low, so a linear relationship is not a very good option to describe the data. 


Eventhough both the box plots (2) and the density plots (3) results could have been good indicators for a linear relationship, the lack of a fitting straight line on the scatter plotc(1) and the low value in the result of the correlation (4) indicate the opposite.

Taking into consideration all results so far for this part, it is not unreasonable to predict that a linear regression will probably not be the best model to describe the relationships in our data.



##   4.2 Linear Regression                                                                                                 ####

### Linear Regression

```{r}
reg2 <- lm(vorinostat ~ BM_Copynumber_meancol, data = lm_tab2)
```


### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg2)
```

Multiple R-squared:  0.004101
This indicates that only 0,4101% percent of the variation in the data (drug sensitivity) can be explained by the relationship between drug sensitivity and copynumber. In other words, there is a 0,4101% variance reduction when we take the copynumber into account. 

p-value: 0.6487 
As the p-value for reg2 is significantly larger than 0.05 and R-squared tells us the copynumber only explains 0,4101% of the variation in the data, it is safe to assume that there is no linear relationship between drug sensitivity and copynumer, a.k.a copynumber cannot predict drug sensitivity. 

#### More information about the fit (linear ecuation: y = y-intercept + slope * x) : 

```{r}
confint(reg2)
```

We can also visualize the results of the confidence interval
```{r}
library(ggplot2)

ggplot(data = lm_tab2, aes(x = BM_Copynumber_meancol, y = vorinostat)) +
  geom_point() +
  stat_smooth(method = "lm", col = "springgreen3", level = 0.975) +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Linear Model Fitted to Data") 
```

Eventhough many points are inside the grey area, there are many more that are scattered across the graph. The confidence interval is very big and the data dispersed. There is no correlation between the variables.  

##   4.3 Checking the normalization of residuals                                                                         

Here we explore two ways to visualize the normalization of residuals.

```{r}
hist(reg2$residuals, 
     breaks = 20,
     xlab = "Residuals", 
     main = "Drug sensitivity vs copynumber: Histogram of the residuals")
```

The graph lets us see that the residuals are not normally distributed. 

```{r}
qqnorm(reg2$residuals, col = "springgreen4", main = "Q-Q plot: Drug sensitivity vs copynumber")
qqline(reg2$residuals, col = "red")
```

Most points of the data seem to meet the red line. However, it is important to take note of the points that are potitioned after the 2nd Quantile, as they are the ones that distance themselves the most from the red line. 

##   4.4 Visualization: Plots that describe the linear regression                                                           


### Residual diagnostics: are the various assumptions that underpin linear regression reasonable for our data?

```{r}
library(lattice)

xyplot(resid(reg2) ~ fitted(reg2),
       xlab = "Fitted Values",
       ylab = "Residuals",
       main = "Residual Diagnostic Plot",
       col = "hotpink1",
       panel = function(x, y, ...)
       {
         panel.grid(h = -1, v = -1)
         panel.abline(h = 0)
         panel.xyplot(x, y, ...)
       }
      )
```


#### Visualization of regression 

```{r}
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2))
plot(reg2, which = c(1, 2))
```

The red line of the Residual vs Fitted graph is a good tool that lets us visualize just how disperse our data is and that a linear model is not a good fit to describe the data. 

#### Comparing prediction and real values for drug sensitivity

```{r}
plot(lm_tab2$vorinostat, 
     reg2$fitted.values, 
     pch = 20, col = "springgreen3", 
     xlab = "Real values", 
     ylab = "Predicted values")
abline(0, 1, col = "red")
```


#  5.  SIMPLE LINEAR REGRESSION WITH ALL GENES: Drug sensitivity with copynumber                                         

Can we predict drug sensitivity using the copynumber data for all genes? How much of the variance of the data can be explained using the copynumber data?

Which differences will we see in our data when we consider the copynumber of all genes, instead of just the biomarkers? 

### Data frames

```{r}
CN_all = as.data.frame(CN_meancol)

DS = as.data.frame(drug_sensitivity)
```

### Table with drug sensitivity and doubling time per cell line

```{r}
lm_tab_all2 = transform(merge(CN_all,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)
lm_tab_all2 <- na.omit(lm_tab_all2)
```


##   5.1 Plots and visualization: Predicting how fit linear regression will be as a model to describe our data              


### Ploting the data: can a linear relationship be observed? Should we have expected a high value for R-squared?

#### Visualization of doubling time vs drug sensitivity

```{r}
scatter.smooth(lm_tab_all2$vorinostat, 
               lm_tab_all2$CN_meancol, 
               col = "orangered",
               main = "Drug sensitivity & Copynumber Regression",
               xlab = "Drug sensitivity",
               ylab = "Copynumber",
               cex = 1.3,
               pch = 1)
```


At first look, the points in the plot are so extremely scattered, that a linear relationship seems really unlikely. The second problem that one can observe in this graphic, is that the line describing the behaviour is not straight. 


#### (2) Box plot

```{r}
par(mfrow=c(1, 2))
boxplot(lm_tab_all2$vorinostat, 
        main="Drug sensitivity") 
boxplot(lm_tab_all2$CN_meancol, 
        main="Copynumber") 
```

No outliers can be observed.

#### (3) Density: Should we expect normality for drug sensitivity?

```{r}
par(mfrow=c(1, 2)) 

plot(density(lm_tab_all2$vorinostat), 
     main="Density Plot: Drug Sensitivity", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab_all2$vorinostat), 2))
    )

polygon(density(lm_tab_all2$vorinostat), col="orangered")
plot(density(lm_tab_all2$CN_meancol), 
     main="Density Plot: Copynumber", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab_all2$CN_meancol), 2))
    )

polygon(density(lm_tab_all2$CN_meancol), col="lightcoral")
```


Skewness of the plot on the left: 0.03 -> Plot is very slightly skewed to the right. 

Skewness of the plot on the right: -0.42 -> Plot is skewed to the left.


#### (4) Correlation: what is the level level of linear dependence between the two variables?

```{r}
cor(lm_tab_all2$vorinostat, lm_tab_all2$CN_meancol)
```

The value here is too low and therefore indicates an extremely weak relationship between both variables. 


Although no outliers were observed in the box plot results, all ther results seem to indicate that a linear regression is most likely not the best model to describe our data. 


##   5.2 Linear Regression                                                                                                 

### Linear Regression

```{r}
reg_all2 <- lm(vorinostat ~ CN_meancol, data = lm_tab_all2)
```


### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_all2)
```

Multiple R-squared:  
0.02355 -> This indicates that only 2.355% percent of the variation in the data (drug sensitivity) can be explained by the relationship between drug sensitivity and copynumber. In other words, there is a 2.355% variance reduction when we take the copynumber into account. 

p-value: 0.2726 
As the p-value for reg_all2 is significantly larger than 0.05 and R-squared tells us the copynumber only explains 2.355% of the variation in the data, it is safe to assume that there is no linear relationship between drug sensitivity and copynumer, a.k.a copynumber cannot predict drug sensitivity. 

#### More information about the fit (linear ecuation: y = y-intercept + slope * x) :
```{r}
confint(reg_all2)
```

We can also visualize the results of the confidence interval
```{r}
library(ggplot2)

ggplot(data = lm_tab_all2, aes(x = CN_meancol, y = vorinostat)) +
  geom_point() +
  stat_smooth(method = "lm", col = "lightcoral", level = 0.975) +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Linear Model Fitted to Data") 
```

##   5.3 Checking the normalization of residuals                                                                           ####

Here we explore two ways to visualize the normalization of residuals.
```{r}
hist(reg_all2$residuals, 
     breaks = 20,
     xlab = "Residuals", 
     main = "Drug sensitivity vs copynumber: Histogram of the residuals")
```


The data does NOT look normally distributed. This menas that the linear regression model does not explain all trends in the dataset.

```{r}
qqnorm(reg_all2$residuals, col = "royalblue1", main = "Q-Q plot: Drug sensitivity vs copynumber")
qqline(reg_all2$residuals, col = "red")
```

Many data points at both extremes do not follow the line that dictates the behaviour in the linear relationship. Therefore, asuming normalization is discarded.

##   5.4 Visualization: Plots that describe the linear regression                                                          ####


#### Residual diagnostics: are the various assumptions that underpin linear regression reasonable for our data?

```{r}
library(lattice)

xyplot(resid(reg_all2) ~ fitted(reg_all2),
       xlab = "Fitted Values",
       ylab = "Residuals",
       main = "Residual Diagnostic Plot",
       col = "hotpink",
       panel = function(x, y, ...)
       {
         panel.grid(h = -1, v = -1)
         panel.abline(h = 0)
         panel.xyplot(x, y, ...)
       }
      )
```


#### Visualization of regression 

```{r}
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2))
plot(reg_all2, which = c(1, 2))
```

Looking at the red line of the graph on the left side, we can confirm the results of the summary of the regression. The low R-squared value is better visualized here. If a linear regression was a good model to explain our data, then the red line would be straight and would go right through the middle of the graph. 

#### Comparing prediction and real values for drug sensitivity

```{r}
plot(lm_tab_all2$vorinostat, 
     reg_all2$fitted.values, 
     pch = 20, col = "lightcoral", 
     xlab = "Real values", 
     ylab = "Predicted values")
abline(0, 1, col = "royalblue1")
```
The data points are very scattered.

#  6.  MULTIPLE REGRESSION WITH ALL GENES: Drug sensitivity with doubling time and copynumber

### Data frames

```{r}
CN_all = as.data.frame(CN_meancol)

DT = as.data.frame(Doubling_Time)

DS = as.data.frame(drug_sensitivity)
```


### Table with drug sensitivity, copynumber and doubling time per cell line

```{r}
lm_tab_m = transform(merge(CN_all, 
                           lm_tab,by=0,all=TRUE), 
                          row.names=Row.names, 
                          Row.names=NULL
                    )

names(lm_tab_m)[names(lm_tab_m) == "CN_meancol"] <- "Copynumber"
names(lm_tab_m)[names(lm_tab_m) == "vorinostat"] <- "Drug_Sensitivity"

lm_tab_m <- na.omit(lm_tab_m)
```


##   6.1 Plots and visualization: Predicting how fit linear regression will be as a model to describe our data            

### Ploting the data: can a linear relationship be observed? Should we have expected a high value for R-squared?

#### Visualization of doubling time vs drug sensitivity

#### Loading packages

```{r}
library(ggplot2)
library(scatterplot3d)
library(car)
library(scatterD3)
library(rgl)
```


#### (1) Plot

```{r}
plot(lm_tab_m , 
     pch=20 , 
     cex=1.5 , 
     col=rgb(0.5, 0.8, 0.9, 0.7)
    )
```

There seems to be no linear relationship, because of how spread the points are.

#### (2) Box plot

```{r}
par(mfrow=c(1, 3))
boxplot(lm_tab_m$Drug_Sensitivity, 
        main="Drug sensitivity") 
boxplot(lm_tab_m$Doubling_Time, 
        main="Doubling time") 
boxplot(lm_tab_m$Copynumber, 
        main="Copynumber") 
```


Just as before we see only outliers for doubling time.

#### (3) Density: Should be expect normality for drug sensitivity?

```{r}
par(mfrow=c(1, 3)) 

plot(density(lm_tab_m$Drug_Sensitivity), 
     main="Density Plot: Drug sensitivity", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab_m$Drug_Sensitivity), 2))
     )

polygon(density(lm_tab_m$Drug_Sensitivity), col="darkorchid1")

plot(density(lm_tab_m$Doubling_Time), 
     main="Density Plot: Doubling time", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab_m$Doubling_Time), 2))
     )

polygon(density(lm_tab_m$Doubling_Time), col="skyblue1")

plot(density(lm_tab_m$Copynumber), 
     main="Density Plot: Copynumber", 
     ylab="Frequency", 
     sub=paste("Skewness:", round(e1071::skewness(lm_tab_m$Copynumber), 2))
     )

polygon(density(lm_tab_m$Copynumber), col="olivedrab1")
```


Skewness of the plot on the left: 0.03 -> Plot is very slightly skewed to the right.

Skewness of the plot on the middle: 1.02 -> Plot is slightly skewed to the left.

Skewness of the plot on the right:: -0.42 -> Plot is slightly skewed to the left.


### Checking for correlation

```{r}
cor(lm_tab_m)
```

None of the values here indicate a strong linear relationship.

Thanks to the outliers, considerably spread plots and skewed density plots, it is not unreasonable to predict that a multiple regression with these parameters will probably not be the best model to describe the relationships in our data.


##   6.2 Linear Regression                                                                                                 

### Linear Regression

```{r}
reg_m <- lm(Drug_Sensitivity ~ Copynumber + Doubling_Time, data = lm_tab_m)
```


### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_m)
```


Multiple R-squared: 0.05949 
This indicates that only 5.949% percent of the variation in the data (drug sensitivity) can be explained by the relationship between drug sensitivity, doubling time and copynumber. In other words, there is a 5.949% variance reduction when we take the both the doubling time and the copynumber into account. 


p-value: 0.2158
As the p-value for reg_m is significantly larger than 0.05 and R-squared tells us the copynumber only explains 2.355% of the variation in the data, it is safe to assume that there is no linear relationship between drug sensitivity and copynumer, a.k.a copynumber cannot predict drug sensitivity. 


 Coefficients:
                   Estimate Std.    Error     t-value   Pr(>|t|)    
   (Intercept)      6.139792       0.115579    53.122    <2e-16 ***
   Copynumber       0.373463       0.834851    0.447     0.6566    
   Doubling_Time   -0.005105       0.002975   -1.716     0.0924

F-statistic (multiple regression) : 1.581
F-statistic (drug sensitivity vs doubling time): 2.609 
F-statistic (drug sensitivity vs copynumber):  0.21

The coefficients show that better results are yielded when using doubling time alone to predict drug sensitivity,than when using both doubling time and copynumber.


### RMSE: A measure of how accurately the model predicts the response

```{r}
n = nrow(lm_tab_m)
rmse = sqrt(1/n * sum(reg_m$residuals^2))
rmse
```

Low values indicate a better fit. This result is surprising as the R-squared and p-value tell us that these variables do not contribute significantly to the variance of the data. 

#### More information about the fit (linear ecuation: y = y-intercept + slope * x) :
```{r}
confint(reg_m)
```


##   6.3 Checking the normalization of residuals

Here we explore two ways to visualize the normalization of residuals.


```{r}
hist(reg_m$residuals, 
     breaks = 20,
     xlab = "Residuals", 
     main = "Drug sensitivity vs copynumber and doubling time: Residuals histogram")
```


The data does NOT look normally distributed.

```{r}
qqnorm(reg_m$residuals, col = "orchid1", main = "Q-Q plot: Drug sensitivity vs copynumber and doubling time")
qqline(reg_m$residuals, col = "royalblue1")
```


##   6.4 Visualization: Plots that describe the linear regression  

#### 3D Plot with Regression Plane

```{r}
#Creating 3D plot
m_3d <- scatterplot3d(lm_tab_m, 
                     type = "h", 
                     color = "royalblue2",
                     angle=55, 
                     pch = 16)
#Adding regression plane
reg_m_3D <- lm(Drug_Sensitivity ~ Copynumber + Doubling_Time, data = lm_tab_m)
m_3d$plane3d(reg_m_3D)

```

This 3D plot allows us to see the plane created for the multivariate regression and the location of data points in relation to to the plane. Most points are either above the plane or below it. This matches our previous results for the multiple regression, from which we conlcuded there is no relevant relationship between the variables. 

### Residual diagnostics: are the various assumptions that underpin linear regression reasonable for our data?

```{r}
xyplot(resid(reg_m) ~ fitted(reg_m),
       xlab = "Fitted Values",
       ylab = "Residuals",
       main = "Residual Diagnostic Plot",
       col = "hotpink",
       panel = function(x, y, ...)
       {
         panel.grid(h = -1, v = -1)
         panel.abline(h = 0)
         panel.xyplot(x, y, ...)
       }
      )

```


#### Visualization of regression 

```{r}
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2))
plot(reg_m, which = c(1, 2))
```
The red line is not straight. 

#### Comparing prediction and real values for drug sensitivity

```{r}
plot(lm_tab_m$Drug_Sensitivity, 
     reg_m$fitted.values, 
     pch = 20, 
     col = "orchid1", 
     xlab = "Real values", 
     ylab = "Predicted values")
abline(0, 1, col = "orange1")
```


#  7.  General Conclusions                                                                                               

The predictions made with the scatter plot, boxplot, density plot and the correlations between the predicted and predicting variable were generally speaking good, as most of these statistical analyses predicted that building a model using linear regression would prove to be not ideal. Once we tested for R-squared and p-value, every regression had extremely low R-quared values, and extremely high p-values. This is of course an undesired result. 

Considering the complexity of the process of gene expression, is not entirely surprising that we cannot predict drug sensitivity in a satisfactory way just by relying on copynumber and/or doubling time. 

Eventhough no linear relationships were found, we did discover that doubling time is a better tool to predict our data for drug sensitivity than copynumber. 


Trying to use a different regression model, would be most likely futile if we take into consideration the results and visualizations of our predictors. Take loess, for example, and think of our scatter plots,a curve does not seem much better fitted to predict our data. Even using other regression models, such as Ridge, that allow more complex curves to be fitted to data, finding a model with both high R-squared and low p-value when we consider our predictors, in particular the scatter plot, seems unlikely. 


# **PART 2** 
##   8.  Using PCA to determine independent variables                                                                      

## Data 

```{r}
lm_tab_pca = lm_tab_m[,c(3,1,2)]
```


##   8.1 PCA

```{r}
pca = prcomp(lm_tab_pca[, -1])
summary(pca)
```

##   8.2 A little correlation exploration                                                                                  ####

```{r}
pairs(lm_tab_pca, col = "turquoise2", pch = 20)
```

```{r}
cor_pca = cor(lm_tab_pca)
heatmap(cor_pca, 
        col = cm.colors(256), 
        margins = c(15, 10), 
        main = "Correlation Heatmap")
```

As expected, we observe a higher correlation between doubling time and drug sensitivity than between copynumber and drug sensitivity

### What are exact values for the correlations?

```{r}
cor(lm_tab_pca)
```


cor(lm_tab_pca$Drug_Sensitivity, lm_tab_pca$Copynumber)

Here we confirm that there is indeed a stronger relationship between drug sensitivity and doubling time. Surprisingly, the strongest correlation that we can observe happens to be the one between doubling time and copynumber. 

### How significatn are these values?

```{r}
cor.test(lm_tab_pca$Drug_Sensitivity, lm_tab_pca$Doubling_Time)
```

```{r}
cor.test(lm_tab_pca$Drug_Sensitivity, lm_tab_pca$Copynumber)
```

```{r}
cor.test(lm_tab_pca$Doubling_Time, lm_tab_pca$Copynumber)
```

Considering these results, the validity of the correlation comes into questions. However, in would still be interesting to explore linear regression with PCA. Therefore, we carry on. 

##   8.3 Linear Regression

```{r}
reg_pca = lm(lm_tab_m$Drug_Sensitivity ~ pca$x)
summary(reg_pca)
```

Multiple R-squared: 0.06419
This indicates that only 6.419% percent of the variation in the data (drug sensitivity) can be explained by the relationship between drug sensitivity, doubling time and copynumber. In other words, there is a 6.419% variance reduction when we take the both the doubling time and the copynumber into account. 


p-value: 0.1904
As the p-value for reg_m is significantly larger than 0.05 and R-squared tells us the copynumber only explains 2.355% of the variation in the data, it is safe to assume that there is no linear relationship between drug sensitivity and copynumer, a.k.a copynumber cannot predict drug sensitivity.

The comparison of the results when we carry out a regular multiple regression vs when we carry out one using the principal component, shows us that while R-squared has a higher value when using PCA, the p-value decreases. 

# **PART 3** 

#  9.  REMOVING THE OUTLIERS: Simple linear regression of all genes for drug sensitivity using doubling time             


##   9.1 Boxplot: removing the outliers

Boxplot for doubling time

```{r}
boxplot(lm_tab$Doubling_Time, 
        main="Doubling time") 
```


What are the values of the outliers?
```{r}
boxplot(lm_tab$Doubling_Time)$out
```

Storing the values of the outliers in a vector
```{r}
outliers_DT <- boxplot(lm_tab$Doubling_Time, plot=FALSE)$out
```

Removing the outliers
```{r}
lm_tab[which(lm_tab$Doubling_Time %in% outliers_DT),]
```

We can see that the outliers correspond to those of the cell lines A498 (renal) and HOP-92 (lung).

Creating a new table without the outliers
```{r}
lm_tab_out <- lm_tab[-which(lm_tab$Doubling_Time %in% outliers_DT),]
```


Checking our results
```{r}
boxplot(lm_tab_out$Doubling_Time, 
        main="Doubling time") 
```

There are no outliers now

##   9.2 Linear Regression removing the outliers

```{r}
reg_out <- lm(vorinostat ~ Doubling_Time, data = lm_tab_out)
```

### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_out)
```

Multiple R-squared has a value of 0.01767. The Multiple R-squared when the outliers are not removed is 0.04235. 

The p-value equals 0.3155. The p-value when the outliers are not removed is 0.1116. The result when the outliers are removed is significantly larger. 


We can already observe that this small change, the removal of outliers, has a significant impact on our results. 


#### More information about the fit (linear ecuation: y = y-intercept + slope * x) : 

```{r}
confint(reg1)
```

```{r}
confint(reg_out)
```

Small changes are observed.

##   9.3 Checking the normalization of residuals                                                                           

### Drug sensitivity vs doubling time: Histogram of the residuals

```{r}
par(mfrow=c(1, 2))
hist(reg1$residuals, 
     breaks = 20, 
     xlab = "Residuals", 
     main = "Histogram with outliers")

hist(reg_out$residuals, 
     breaks = 20, 
     xlab = "Residuals", 
     main = "Histogram without outliers")
```

The data in both histograms does not appear normalized and it is hard to say whether there is a case in which the data looks more normally distributed or not.

```{r}
par(mfrow=c(1, 2))

qqnorm(reg1$residuals, 
       main = "Q-Q plot with outliers",
       col = "royalblue1")
qqline(reg1$residuals, 
       col = "red")

qqnorm(reg_out$residuals, 
       main = "Q-Q plot without outliers",
       col = "royalblue3")
qqline(reg_out$residuals,
       col = "red")
```

Small changes can be observed between values 1 and 2.

##   9.4 Visualization: Plots that describe the linear regression                                                          

### Residual diagnostics: are the various assumptions that underpin linear regression reasonable for our data?

```{r}

par(mfrow=c(1, 2))

xyplot(resid(reg1) ~ fitted(reg1),
       xlab = "Fitted Values",
       ylab = "Residuals",
       main = "Residual Diagnostic Plot",
       col = "slateblue3",
       panel = function(x, y, ...)
       {
         panel.grid(h = -1, v = -1)
         panel.abline(h = 0)
         panel.xyplot(x, y, ...)
       }
      )



xyplot(resid(reg_out) ~ fitted(reg_out),
       xlab = "Fitted Values",
       ylab = "Residuals",
       main = "Residual Diagnostic Plot",
       col = "slateblue3",
       panel = function(x, y, ...)
       {
         panel.grid(h = -1, v = -1)
         panel.abline(h = 0)
         panel.xyplot(x, y, ...)
       }
      )
```


The data for reg_out appears to be more spread.


#### Visualization of regression 

Plots with outliers are on top and those without are on the bottom.
```{r}
par(mar = c(4, 4, 2, 2), mfrow = c(2, 2))
plot(reg1, which = c(1, 2))
plot(reg_out, which = c(1, 2))
```

The line drawn accross the scatter plot changes when we remove the outliers.

#### Comparing prediction and real values for drug sensitivity

```{r}
par(mfrow=c(1, 2))

plot(lm_tab$vorinostat, 
     reg1$fitted.values, 
     pch = 20, 
     col = "royalblue1", 
     xlab = "Real values", 
     ylab = "Predicted values",
     main = "Plot with outliers")
abline(0, 1, col = "red")

plot(lm_tab_out$vorinostat, 
     reg_out$fitted.values, 
     pch = 20, 
     col = "royalblue3", 
     xlab = "Real values", 
     ylab = "Predicted values",
     main = "Plot without outliers")
abline(0, 1, col = "red")

```

Changes in the red line can be observed: change in the slope and intercept with the x-axis.

##   9.5  General Conclusions                                                                                               

There are small differences in the plots when the outliers are removed. The relationship between both variables gets more uncertain when the outliers are removed. As the R-Squared value decreases and p-value increases, it is not safe to say whether removing the outliers is better or not. The resukts of this small exploratory analysis do not allow us to know which option will render more truthfull results. The conclusion we can draw is that outliers have a powerful effect on the end results of a statistical analysis and that they should be considered when making any kind of analysis.

# **PART 4** 

#  10.  Comparing the results for the linear regression of drug sensitivity vs copynumber using only the biomarkers wvs using all genes.              


While the value of R-squared when using the biomarkers is of 0.004101 and the p-value is equal to 0.6487, when all genes are used instead R-squared has a value of 0.02355 and the p-value is 0.2726.

R-squared is significantly greater when using all genes and the p-value is significantly smaller. Therefore better results are expected when using all genes. 

# 11. Linear Regression with specific genes: Drug Sensitivity vs Copynumber (#anchor37)

## 11.1. Linear Regression with top 3 biomarkers (#anchor38)

Here we use the first 3 rows of the table BM_Copynumber, which contains the copynumber values for the top 100 biomarkers.Using the table FC_both_sorted, which contains the biomarkers in decreasing order of importance. The genes we analyse in this part are DHRS2, ABAT and SERPINI1.

### Linear Regression for DHRS2: Preparing the data

First we take the copynumber values of this gene only.
```{r}
DHRS2 <- BM_copynumber[-c(2:90),] 
```

#### Data frames

```{r}
DHRS2 = as.data.frame(t(DHRS2))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene per cell line

```{r}
lm_tab_DHRS2 = transform(merge(DHRS2,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_DHRS2 <- na.omit(lm_tab_DHRS2)
```

#### Linear Regression for DHRS2
```{r}
reg_DHRS2 <- lm(vorinostat ~ DHRS2, data = lm_tab_DHRS2)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_DHRS2)
```

***Multiple R-squared**:  1.267e-06
***p-value**: 0.9936


### Linear Regression for ABAT: Preparing the data

First we take the copynumber values of this gene only.
```{r}
ABAT <- Copynumber["ABAT",]
```

#### Data frames

```{r}
ABAT = as.data.frame(t(ABAT))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_ABAT = transform(merge(ABAT,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_ABAT <- na.omit(lm_tab_ABAT)
```

#### Linear Regression for ABAT
```{r}
reg_ABAT <- lm(vorinostat ~ ABAT, data = lm_tab_ABAT)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_ABAT)
```

***Multiple R-squared**:  0.05358
***p-value**: 0.09537

### Linear Regression for SERPINI1: Preparing the data

First we take the copynumber values of this gene only.
```{r}
SERPINI1 <- Copynumber["SERPINI1",]
```

#### Data frames

```{r}
SERPINI1 = as.data.frame(t(SERPINI1))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_SERPINI1 = transform(merge(SERPINI1,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_SERPINI1 <- na.omit(lm_tab_SERPINI1)
```

#### Linear Regression for SERPINI1
```{r}
reg_SERPINI1 <- lm(vorinostat ~ SERPINI1, data = lm_tab_SERPINI1)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_SERPINI1)
```

***Multiple R-squared**:  0.006903
***p-value**: 0.5542

## 11.2. Linear Regression with 3 non-biomarker genes (#anchor39)

Here we use the functions to sort biomarkers according to their mean value to check which genes are the least relevant ones in terms of change of expression after treatment with vorinostat.

### Linear Regression for CABP2: Preparing the data

First we take the copynumber values of this gene only.
```{r}
CABP2 <- Copynumber["CABP2",]
```

#### Data frames

```{r}
CABP2 = as.data.frame(t(CABP2))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_CABP2 = transform(merge(CABP2,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_CABP2 <- na.omit(lm_tab_CABP2)
```

#### Linear Regression for CABP2
```{r}
reg_CABP2 <- lm(vorinostat ~ CABP2, data = lm_tab_CABP2)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_CABP2)
```

***Multiple R-squared**:  8.001e-05
***p-value**: 0.9493

### Linear Regression for UPK2: Preparing the data

First we take the copynumber values of this gene only.
```{r}
UPK2 <- Copynumber["UPK2",]
```

#### Data frames

```{r}
UPK2 = as.data.frame(t(UPK2))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_UPK2 = transform(merge(UPK2,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_UPK2 <- na.omit(lm_tab_UPK2)
```

#### Linear Regression for UPK2
```{r}
reg_UPK2 <- lm(vorinostat ~ UPK2, data = lm_tab_UPK2)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_UPK2)
```

***Multiple R-squared**:  0.001952
***p-value**: 0.7534

### Linear Regression for CLPS: Preparing the data

First we take the copynumber values of this gene only.
```{r}
CLPS <- Copynumber["CLPS",]
```

#### Data frames

```{r}
CLPS = as.data.frame(t(CLPS))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_CLPS = transform(merge(CLPS,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_CLPS <- na.omit(lm_tab_CLPS)
```

#### Linear Regression for CLPS
```{r}
reg_CLPS <- lm(vorinostat ~ CLPS, data = lm_tab_CLPS)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_CLPS)
```

***Multiple R-squared**:  0.007549
***p-value**: 0.5362


       
## 11.3. Linear Regression with genes with the highest and lowest mean values for copynumber (#anchor40)


In this part of this analysis, we find find the genes with the highest and lowest values for copynumber. To achieve this, we find the mean of the absolute values and then sort the results in decresing order. 

```{r}
Copynumber_av= abs(Copynumber)
Copynumber_mean= rowMeans(Copynumber_av)

Copynumber_sav <- sort(Copynumber_mean, decreasing = TRUE)
Copynumber_sav <- as.matrix(Copynumber_sav)
```

####Top 10
```{r}
CN_top10 = Copynumber_sav[1:10,]
CN_top10
```

####Lowest 10
```{r}
CN_lowest10 = Copynumber_sav[23306:23316,]
CN_lowest10
```



### 11.3.1. Top 3 mean values for Copynumber (#anchor41)

### Linear Regression for DAZ2: Preparing the data

First we take the copynumber values of this gene only.
```{r}
DAZ2 <- Copynumber["DAZ2",]
```

#### Data frames

```{r}
DAZ2 = as.data.frame(t(DAZ2))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_DAZ2 = transform(merge(DAZ2,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_DAZ2 <- na.omit(lm_tab_DAZ2)
```

#### Linear Regression for DAZ2
```{r}
reg_DAZ2 <- lm(vorinostat ~ DAZ2, data = lm_tab_DAZ2)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_DAZ2)
```

***Multiple R-squared**:  0.0009194
***p-value**: 0.8294

### Linear Regression for UTY: Preparing the data

First we take the copynumber values of this gene only.
```{r}
UTY <- Copynumber["UTY",]
```

#### Data frames

```{r}
UTY = as.data.frame(t(UTY))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_UTY = transform(merge(UTY,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_UTY <- na.omit(lm_tab_UTY)
```

#### Linear Regression for UTY
```{r}
reg_UTY <- lm(vorinostat ~ UTY, data = lm_tab_UTY)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_UTY)
```

***Multiple R-squared**:  0.001045
***p-value**: 0.8182

### Linear Regression for DAZ1: Preparing the data

First we take the copynumber values of this gene only.
```{r}
DAZ1 <- Copynumber["DAZ1",]
```

#### Data frames

```{r}
DAZ1 = as.data.frame(t(DAZ1))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_DAZ1 = transform(merge(DAZ1,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_DAZ1 <- na.omit(lm_tab_DAZ1)
```

#### Linear Regression for DAZ1
```{r}
reg_DAZ1 <- lm(vorinostat ~ DAZ1, data = lm_tab_DAZ1)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_DAZ1)
```

***Multiple R-squared**:  0.003956
***p-value**: 0.6546


       
### 11.3.2. Lowest 3 mean values for copynumber (#anchor42)

### Linear Regression for RBM18: Preparing the data

First we take the copynumber values of this gene only.
```{r}
RBM18 <- Copynumber["RBM18",]
```

#### Data frames

```{r}
RBM18 = as.data.frame(t(RBM18))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_RBM18 = transform(merge(RBM18,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_RBM18 <- na.omit(lm_tab_RBM18)
```

#### Linear Regression for RBM18
```{r}
reg_RBM18 <- lm(vorinostat ~ RBM18, data = lm_tab_RBM18)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_RBM18)
```

***Multiple R-squared**:  6.488e-07
***p-value**: 0.9954

### Linear Regression for MIR3974: Preparing the data

First we take the copynumber values of this gene only.
```{r}
MIR3974 <- Copynumber["MIR3974",]
```

#### Data frames

```{r}
MIR3974 = as.data.frame(t(MIR3974))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_MIR3974 = transform(merge(MIR3974,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_MIR3974 <- na.omit(lm_tab_MIR3974)
```

#### Linear Regression for MIR3974
```{r}
reg_MIR3974 <- lm(vorinostat ~ MIR3974, data = lm_tab_MIR3974)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_MIR3974)
```

***Multiple R-squared**:  0.01483
***p-value**: 0.3851

### Linear Regression for SKP1P2: Preparing the data

First we take the copynumber values of this gene only.
```{r}
SKP1P2 <- Copynumber["SKP1P2",]
```

#### Data frames

```{r}
SKP1P2 = as.data.frame(t(SKP1P2))

DS = as.data.frame(drug_sensitivity)
```


#### Table with drug sensitivity and relevant gene copynumber per cell line

```{r}
lm_tab_SKP1P2 = transform(merge(SKP1P2,DS,by=0,all=TRUE), row.names=Row.names, Row.names=NULL)

lm_tab_SKP1P2 <- na.omit(lm_tab_SKP1P2)
```

#### Linear Regression for SKP1P2
```{r}
reg_SKP1P2 <- lm(vorinostat ~ SKP1P2, data = lm_tab_SKP1P2)
```

#### Details about the linear regression: what we need draw some conclusions

```{r}
summary(reg_SKP1P2)
```

***Multiple R-squared**:  0.01483
***p-value**: 0.3851



# 12. Table with summary (#anchor43)

